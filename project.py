# -----------------------------------------------------------
# 1. KÃ¼tÃ¼phane Ä°Ã§e Aktarma ve Ayarlar (Lokal VektÃ¶rleme Eklendi)
# -----------------------------------------------------------
import os
import glob
from dotenv import load_dotenv 
import streamlit as st

# Lokal Embedding Modeli iÃ§in kÃ¼tÃ¼phane (API limitini aÅŸar)
from langchain_community.embeddings import SentenceTransformerEmbeddings 

# DiÄŸer LangChain BileÅŸenleri
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.document_loaders import TextLoader 
from langchain_community.document_loaders import PyPDFLoader 
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain.chains import RetrievalQA

# .env dosyasÄ±ndaki GOOGLE_API_KEY'i yÃ¼kle
load_dotenv() 

st.set_page_config(layout="wide")

# -----------------------------------------------------------
# 2. RAG Pipeline Kurulumu (Lokal VektÃ¶rleme)
# -----------------------------------------------------------

@st.cache_resource
def setup_rag_pipeline():
    st.info("Veriler YÃ¼kleniyor ve LOKAL OLARAK VektÃ¶rleniyor. LÃ¼tfen Bekleyin...")

    # Dosya YÃ¼kleme (PDF ve TXT)
    txt_files = glob.glob("masallar/*.txt")
    pdf_files = glob.glob("masallar/*.pdf")
    documents = []
    
    # YÃ¼kleme iÅŸlemleri...
    for path in txt_files:
        try:
            loader = TextLoader(path, encoding='utf-8')
            documents.extend(loader.load())
        except Exception as e:
            st.error(f"TXT yÃ¼klenirken hata: {path}, Hata: {e}")

    for path in pdf_files:
        try:
            loader = PyPDFLoader(path)
            documents.extend(loader.load())
        except Exception as e:
            st.error(f"PDF yÃ¼klenirken hata: {path}, Hata: {e}")

    # 1. Metin ParÃ§alama (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
        length_function=len
    )
    texts = text_splitter.split_documents(documents)
    
    # 2. LOKAL VektÃ¶rleme (Embedding) - ARTIK API KOTASINI AÅMAYACAK!
    embedding_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # 3. VektÃ¶r VeritabanÄ±
    vectorstore = DocArrayInMemorySearch.from_documents(texts, embedding_model)
    
    st.success(f"Veri YÃ¼kleme ve Lokal VektÃ¶rleme TamamlandÄ±. ParÃ§a SayÄ±sÄ±: {len(texts)}")
    
    # 4. LLM ve RetrievalQA Zinciri (Sadece cevap Ã¼retimi iÃ§in Gemini API'si kullanÄ±lÄ±r)
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4})
    )
    
    return qa_chain

# RAG Pipeline'Ä± kur
qa_chain = setup_rag_pipeline()


# -----------------------------------------------------------
# 3. Streamlit ArayÃ¼zÃ¼ ve Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±
# -----------------------------------------------------------

st.title("ğŸ“š Masal DiyarÄ± Bilgi AsistanÄ± (Lokal VektÃ¶rleme)")
st.markdown("Merhaba! Lokal vektÃ¶rleme kullanarak Ã§alÄ±ÅŸan geliÅŸmiÅŸ RAG asistanÄ±dÄ±r.")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Hangi masal hakkÄ±nda soru sormak istersiniz?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.spinner("Masal DiyarÄ±'nda cevap aranÄ±yor..."):
        try:
            # Sadece tek bir Ã§aÄŸrÄ± yapÄ±yoruz: QA zincirini Ã§aÄŸÄ±rma
            result = qa_chain.invoke({"query": prompt})
            generated_text = result['result']
            source_info = "\n\n**Cevap, yÃ¼klenen masal metinlerine dayanÄ±larak Ã¼retilmiÅŸtir.**"
            full_response = generated_text + source_info

        except Exception as e:
            full_response = f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu. LÃ¼tfen API AnahtarÄ±nÄ±zÄ± ve internet baÄŸlantÄ±nÄ±zÄ± kontrol edin. Hata: {e}"
            
    with st.chat_message("assistant"):
        st.markdown(full_response)
    st.session_state.messages.append({"role": "assistant", "content": full_response})
